<?xml version="1.0"?>
<job_conf>
    <plugins workers="2">
        <!-- "workers" is the number of threads for the runner's work queue.
             The default from <plugins> is used if not defined for a <plugin>.
          -->
        <plugin id="local" type="runner" load="galaxy.jobs.runners.local:LocalJobRunner"/>
        <plugin id="drmaa" type="runner" load="galaxy.jobs.runners.drmaa:DRMAAJobRunner">
            <!-- Different DRMs handle successfully completed jobs differently,
                 these options can be changed to handle such differences and
                 are explained in detail on the Galaxy wiki. Defaults are shown -->
            <param id="invalidjobexception_state">ok</param>
            <param id="invalidjobexception_retries">0</param>
            <param id="internalexception_state">ok</param>
            <param id="internalexception_retries">0</param>
        </plugin>
        <plugin id="cli" type="runner" load="galaxy.jobs.runners.cli:ShellJobRunner" />
        <plugin id="slurm" type="runner" load="galaxy.jobs.runners.slurm:SlurmJobRunner" />
    </plugins>
    <handlers default="handlers">
        <!-- Additional job handlers - the id should match the name of a
             [server:<id>] in galaxy.ini.
         -->
        <handler id="handler0" tags="handlers"/>
        <handler id="handler1" tags="handlers"/>
        <!-- Handlers will load all plugins defined in the <plugins> collection
             above by default, but can be limited to a subset using <plugin>
             tags. This is useful for heterogenous environments where the DRMAA
             plugin would need to be loaded more than once with different
             configs.
         -->
    </handlers>
    <destinations default="local">
        <!-- Destinations define details about remote resources and how jobs
             should be executed on those remote resources.
         -->
        <destination id="local" runner="local"/>
        <destination id="multicore_local" runner="local">
          <param id="local_slots">4</param> <!-- Specify GALAXY_SLOTS for local jobs. -->
          <!-- Warning: Local slot count doesn't tie up additional worker threads, to prevent over
               allocating machine define a second local runner with different name and fewer workers
               to run this destination. -->
          <param id="embed_metadata_in_job">True</param>
          <!-- Above parameter will be default (with no option to set
               to False) in an upcoming release of Galaxy, but you can
               try it early - it will slightly speed up local jobs by
               embedding metadata calculation in job script itself.
          -->
          <job_metrics />
          <!-- Above element demonstrates embedded job metrics definition - see
               job_metrics_conf.xml.sample for full documentation on possible nested
               elements. This empty block will simply disable job metrics for the
               corresponding destination. -->
        </destination>
        <!-- Example CLI Slurm runner. -->
        <destination id="ssh_slurm" runner="cli">
            <param id="shell_plugin">SecureShell</param>
            <param id="job_plugin">Slurm</param>
            <param id="shell_username">foo</param>
            <param id="shell_hostname">my_host</param>
            <param id="job_time">2:00:00</param>
            <param id="job_ncpus">4</param>
            <param id="job_partition">my_partition</param>
        </destination>

        <destination id="condor" runner="condor">
            <!-- With no params, jobs are submitted to the 'vanilla' universe with:
                    notification = NEVER
                    getenv = true
                 Additional/override query ClassAd params can be specified with
                 <param> tags.
            -->
            <param id="request_cpus">8</param>

            <!-- Recent version of HTCondor do have a `docker` universe to handle containers.
                 Activate this feature by explicitly specifying the `docker` universe.
            -->
            <!-- <param id="universe">docker</param> -->

            <!-- If the tool has a container specified this one is used.

                <requirements>
                    <container type="docker">bgruening/galaxy-stable</container>
                </requirements>

            Unless the job destination specifies an override 
            with docker_container_id_override. If neither of 
            these is set a default container can be specified
            with docker_default_container_id. The resolved
            container ID will be passed along to condor as
            the docker_image submission parameter.
            -->
            <!-- <param id="docker_default_container_id">busybox:ubuntu-14.04</param> -->
        </destination>

        <!-- Jobs can be re-submitted for various reasons (to the same destination or others,
             with or without a short delay). For instance, jobs that hit the walltime on one
             destination can be automatically resubmitted to another destination. Re-submission
             is defined on a per-destination basis using ``resubmit`` tags. Re-submission only
             happens currently in response to problems in the job runner - so for instance if a
             job fails to allocate memory but the job runner doesn't detect this and completes
             the job normally but the exit code indicates the error - the job failure
             re-submission won't run yet (this will be added in the future).

             Multiple `resubmit` tags can be defined, the first resubmit condition that is true
             (i.e. evaluates to a Python truthy value) will be used for a particular job failure.

             The ``condition`` attribute is optional, if not present, the
             resubmit destination will be used for all relevant failure types.
             Conditions are expressed as Python-like expressions (a fairly safe subset of Python
             is available). These expressions include math and logical operators, numbers,
             strings, etc.... The following variables are available in these expressions:

               - "walltime_reached" (True if and only if the job runner indicates a walltime maximum was reached)
               - "memory_limit_reached" (True if and only if the job runner indicates a memory limit was hit)
               - "unknown_error" (True for job or job runner problems that aren't otherwise classified)
               - "attempt" (the re-submission attempt number this is)
               - "seconds_since_queued" (the number of seconds since the last time the job was in a queued state within Galaxy)
               - "seconds_running" (the number of seconds the job was in a running state within Galaxy)

             The ``handler`` attribute is optional, if not present, the job's original
             handler will be reused for the resubmitted job. The ``destination`` attriubte
             is optional, if not present the job's original destination will be reused for the
             re-submission. The ``delay`` attribute is optional, if present it will cause the job to
             delay for that number of seconds before being re-submitted. 
        -->
        <destination id="short_fast" runner="slurm">
            <param id="nativeSpecification">--time=00:05:00 --nodes=1</param>
            <resubmit condition="walltime_reached" destination="long_slow" handler="sge_handler" />
        </destination>
        <destination id="long_slow" runner="sge">
            <!-- The destination that you resubmit jobs to can be any runner type -->
            <param id="nativeSpecification">-l h_rt=96:00:00</param>
        </destination>
        <destination id="smallmem" runner="slurm">
            <param id="nativeSpecification">--mem-per-cpu=512</param>
            <resubmit condition="memory_limit_reached" destination="bigmem" />
        </destination>
        <destination id="retry_on_unknown_problems" runner="slurm">
            <!-- Just retry the job 5 times if un-categories errors occur backing
                 off by 30 more seconds between attempts. -->
            <resubmit condition="unknown_error and attempt &lt;= 5" delay="attempt * 30" />
        </destination>
        <!-- Any tag param in this file can be set using an environment variable or using
             values from galaxy.ini using the from_environ and from_config attributes
             repectively. The text of the param will still be used if that environment variable
             or config value isn't set.
        -->
        <destination id="params_from_environment" runner="slurm">
            <param id="nativeSpecification" from_environ="NATIVE_SPECIFICATION">--time=00:05:00 --nodes=1</param>
            <param id="docker_enabled" from_config="use_docker">false</param>
        </destination>

        <destination id="my-tool-container" runner="k8s">
            <!-- For the kubernetes (k8s) runner, each container is a destination.

                 Make sure that the container is able to execute the calls that will be passed by the galaxy built
                 command. Most notably, containers that execute scripts through an interpreter in the form
                 Rscript my-script.R <arguments>
                 should have this wrapped as the container set working directory won't be the one actually used by
                 galaxy (galaxy creates a new working director and moves to it). Recommendation is hence to wrap this
                 type of calls on a shell script, and leave that script with execution privileges on the PATH of the
                 container:

                 RUN echo '#!/bin/bash' > /usr/local/bin/myScriptExec
                 RUN echo 'Rscript /path/to/my-script.r "$@"' >> /usr/local/bin/myScriptExec
                 RUN chmod a+x /usr/local/bin/myScriptExec

            -->

            <!-- The following four fields assemble the container's full name:
                 docker pull <repo>/<owner>/<image>:tag
            -->
            <param id="docker_repo_override">my-docker-registry.org</param>
            <param id="docker_owner_override">superbioinfo</param>
            <param id="docker_image_override">my-tool</param>
            <param id="docker_tag_override">latest</param>
            <!-- Alternatively you could specify a different type of container, such as rkt (not tested with Kubernetes)
                <param id="rkt_repo_override">my-docker-registry.org</param>
                <param id="rkt_owner_override">superbioinfo</param>
                <param id="rkt_image_override">my-tool</param>
                <param id="rkt_tag_override">latest</param>
            -->
            <!-- You can also allow the destination to accept the docker container set in the tool, and only fall into
                 the docker image set by this destination if the tool doesn't set a docker container, by using the
                 "default" suffix instead of "override".
                <param id="docker_repo_default">my-docker-registry.org</param>
                <param id="docker_owner_default">superbioinfo</param>
                <param id="docker_image_default">my-tool</param>
                <param id="docker_tag_default">latest</param>
            -->
            <param id="max_pod_retrials">3</param>
            <!-- Allows pods to retry up to this number of times, before marking the galaxy job failed. k8s is a state
                 setter essentially, so by default it will try to take a job submitted to successful completion. A job
                 submits pods, until the number of successes (1 in this use case) is achieved, assuming that whatever is
                 making the pods fail will be fixed (such as a stale disk or a dead node that it is being restarted).
                 This option sets a limit of retrials, so that after that number of failed pods, the job is re-scaled to
                 zero (no execution) and the stderr/stdout of the k8s job is reported in galaxy (and the galaxy job set
                 to failed).

                 Overrides the runner config. (Not implemented yet)
            -->
            <!-- REQUIRED: To play nicely with the existing galaxy setup for containers. This could be set though
                 internally by the runner. -->
            <param id="docker_enabled">true</param>
        </destination>
        <destination id="god" runner="godocker">
            <!-- The following are configurations for the container -->
            <param id="docker_enabled">true</param>
            <param id="docker_cpu">1</param>
            <param id="docker_memory">2</param>
            <param id="docker_default_container_id">centos:latest</param>
            <!-- Specify the image on which the jobs have to be executed -->
            <param id="godocker_volumes"></param>
            <!-- Mount the godocker volumes 
                 volumes must be separated by commas.
                 eg: <param id="godocker_volumes">home,galaxy</param>
            -->
            <param id="virtualenv">false</param>
            <!-- If a tool execution in container requires galaxy virtualenv, 
                 then enable it by setting the value to true.
                 Disable venv by setting the value to false.
            -->
        </destination>
        <destination id="chronos_dest" runner="chronos">
            <param id="docker_enabled">true</param>
            <param id="docker_memory">512</param>
            <param id="docker_cpu">2</param>
            <param id="volumes">/directory/</param>
            <!-- Directory which is mounted to the container and is parent of
                 the  `job_working_directory`, `file_path`, `new_file_path`
                 directories. Directories of the data used by tools are
                 included as well.-->
            <param id="max_retries">2</param>
            <!-- Number of retries to attempt if a command returns a non-zero status -->
        </destination>

        <!-- Templatized destinations - macros can be used to create templated
        destinations with reduced XML duplication. Here we are creating 4 destinations in 4 lines instead of 28 using the macros defined below.
        -->
        <expand macro="foohost_destination" id="foo_small" ncpus="1" walltime="1:00:00" />
        <expand macro="foohost_destination" id="foo_medium" ncpus="2" walltime="4:00:00" />
        <expand macro="foohost_destination" id="foo_large" ncpus="8" walltime="24:00:00" />
        <expand macro="foohost_destination" id="foo_longrunning" ncpus="1" walltime="48:00:00" />
    </destinations>
    <resources default="default">
      <!-- Group different parameters defined in job_resource_params_conf.xml
           together and assign these groups ids. Tool section below can map
           tools to different groups. This is experimental functionality!
      -->
      <group id="default"></group>
      <group id="memoryonly">memory</group>
      <group id="all">processors,memory,time,project</group>
    </resources>
    <tools>
        <!-- Tools can be configured to use specific destinations or handlers,
             identified by either the "id" or "tags" attribute.  If assigned to
             a tag, a handler or destination that matches that tag will be
             chosen at random.
         -->
        <tool id="foo" handler="trackster_handler">
            <param id="source">trackster</param>
        </tool>
        <tool id="bar" destination="dynamic"/>
        <!-- Next example defines resource group to insert into tool interface
             and pass to dynamic destination (as resource_params argument). -->
        <tool id="longbar" destination="dynamic" resources="all" />
        <tool id="baz" handler="special_handlers" destination="bigmem"/>

        <!-- Finally for Kubernetes runner, the following connects a particular tool to be executed with
             the container of choice in Kubernetes.
        -->
        <tool id="my-tool" destination="my-tool-container"/>
    </tools>
    <limits>
        <!-- Certain limits can be defined. The 'concurrent_jobs' limits all
             control the number of jobs that can be "active" at a time, that
             is, dispatched to a runner and in the 'queued' or 'running'
             states.

             A race condition exists that will allow destination_* concurrency
             limits to be surpassed when multiple handlers are allowed to
             handle jobs for the same destination. To prevent this, assign all
             jobs for a specific destination to a single handler.
        -->
        <!-- registered_user_concurrent_jobs:
                Limit on the number of jobs a user with a registered Galaxy
                account can have active across all destinations.
        -->
        <limit type="registered_user_concurrent_jobs">2</limit>
        <!-- anonymous_user_concurrent_jobs:
                Likewise, but for unregistered/anonymous users.
        -->
        <limit type="anonymous_user_concurrent_jobs">1</limit>
        <!-- destination_user_concurrent_jobs:
                The number of jobs a user can have active in the specified
                destination, or across all destinations identified by the
                specified tag. (formerly: concurrent_jobs)
        -->
        <limit type="destination_user_concurrent_jobs" id="local">1</limit>
        <limit type="destination_user_concurrent_jobs" tag="mycluster">2</limit>
        <limit type="destination_user_concurrent_jobs" tag="longjobs">1</limit>
        <!-- destination_total_concurrent_jobs:
                The number of jobs that can be active in the specified
                destination (or across all destinations identified by the
                specified tag) by any/all users.
        -->
        <limit type="destination_total_concurrent_jobs" id="local">16</limit>
        <limit type="destination_total_concurrent_jobs" tag="longjobs">100</limit>
        <!-- walltime:
                Amount of time a job can run (in any destination) before it
                will be terminated by Galaxy.
         -->
        <limit type="walltime">24:00:00</limit>
	<!-- total_walltime:
		Total walltime that jobs may not exceed during a set period.
		If total walltime of finished jobs exceeds this value, any
		new jobs are paused.  `window` is a number in days,
		representing the period.
         -->
        <limit type="total_walltime" window="30">24:00:00</limit>
        <!-- output_size:
                Size that any defined tool output can grow to before the job
                will be terminated. This does not include temporary files
                created by the job. Format is flexible, e.g.:
                '10GB' = '10g' = '10240 Mb' = '10737418240'
        -->
        <limit type="output_size">10GB</limit>
    </limits>
    <macros>
        <xml name="foohost_destination" tokens="id,walltime,ncpus">
            <destination id="@ID@" runner="cli">
                <param id="shell_plugin">SecureShell</param>
                <param id="job_plugin">Torque</param>
                <param id="shell_username">galaxy</param>
                <param id="shell_hostname">foohost_destination.example.org</param>
                <param id="job_Resource_List">walltime=@WALLTIME@,ncpus=@NCPUS@</param>
            </destination>
        </xml>
    </macros>
</job_conf>
